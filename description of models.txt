XGBmodel & XGBmodel_101_feature были получены:
1) первоначальная обработка df (удаление дубликатов, строк где не определенна таргетная переменная)
2) полный отказ от категориальных переменных а также от has_metro
3) в 101_feature были дополнительно убраны колонки где процент пропущенных значений >= 50%
4) деление на train/test в соотношении 90/10
5) обучение xgboost с eval_metric='mae' с использованием kfold, cross_val_score на всем train (Mean MAE: 0.021 (0.022)) #в скобках - mean (std)
6) после валидации на test, обучение на всем датасете X,y (Mean MAE: 0.014 (0.017)) 
7) сохранение моделей с помощью joblib